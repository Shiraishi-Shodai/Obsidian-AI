---
tags:
  - "#DeepLearning"
  - "#CNN"
created:
  - 2025-02-06 14:07
---

## Feeling Notes
---
-[[Position absorption in the convolution and pooling layers]]
## Literature Notes
---
- [[Neural Network]]
- [[Deep Learning]]
## References
---
- [CNNの画像認識手法とは](http://gagbot.net/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD%E8%AC%9B%E5%BA%A716)
- [第20回　畳み込みニューラルネットワーク](https://docs.google.com/presentation/d/1oGFvymUWCEJ3iG3bNtst1LM9VtKvFtKyg_ULSlDIkvA/edit#slide=id.g318a4047390_0_31)
- [畳み込みニューラルネットワーク_CNN(Vol.16)](https://products.sint.co.jp/aisia/blog/vol1-16)

# CNN
---
人間の視覚情報の処理方法をもとに考えられた画像処理に強いニューラルネットワーク
ネットワークが見つけた特徴を各層に保持する

人間の視覚情報
単純型細胞(S細胞)：画像の特徴を抽出
複雑が細胞(C細胞)：空間的な位置のずれを吸収する

全結合とは異なり、**画像の形を維持したまま学習できるためより本質的な特徴を学習**しやすくパラメータ数も比較的少ないため計算しやすい

## CNNを構築する主な層
---
### 1. 畳み込み層
---
フィルターと画像の内積をとることで特徴(輪郭)を抽出する層。

フィルターという(N * N)の重みとバイアス(1 * 1)を使って画像の特徴を抽出する層
畳み込み演算ではフィルターと入力データの各要素の加重和を取る

- 画像の近くにあるピクセル同士が関連しているという空間的な情報を持つ(局所性)
- 各チャンネルに対してフィルターを適応する
- 各チャンネルのフィルター(重み)とバイアスは再利用される(これが各層が学習した特徴を保持する仕組み)
- 計算コストが高い

### 2. プーリング層
---
本質的な情報を残しつつデータを圧縮する層。
モザイクをかけてざっくり学習しやすくする層

- 学習するパラメータがない
- 計算不可が減る
- 小さな変化やノイズを吸収するため、位置の細かい違いに対してロバスト(より強い移動不変性)
- 過学習を防ぐ
- 各チャンネルに対してフィルターを適応する
- 計算コストが低い

**注意点**
- 抽出したい特徴量を失う恐れがある
- 圧縮しすぎると、モデルの表現力に欠ける

### 3. 全結合層
---
前の層の出力を入力として全て受け取る層。
分類の出力層に使用される

### 局所性
---
画像の各ピクセルは近くのピクセルと強い関連があるという性質。
畳み込み層では、フィルターと入力データとの加重和を取ることでこの性質を得ている
例：物の輪郭はだいたい同じ色であり、関連がある

### 移動不変性
---
抽出したい特徴が入力データのどこにあっても検知できる仕組み。

プーリング層では、フィルターを適用したデータを大まかに捉えることでこの特徴を得ている

### ストライド
---
フィルターをスライドする距離

#### パディング
---
入力データの周囲に固定のデータを埋め込むこと。
これで層を増やしても畳み込み演算による画像の圧縮を防ぐことができる


## 疑問
---
[[Position absorption in the convolution and pooling layers | 学校の資料DL 20回の資料で以下のような説明がありましたが、これはそれぞれ具体的に何を意味しているのでしょうか？]] 
> 畳み込み層は「特徴の位置が少し変わっても正確に認識する」(P19)。   
> プーリング層では「画像内で重要な情報を抽出しつつ、位置の微細な違いに対してロバストにする。」(P28)