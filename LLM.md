
# 大規模言語モデル
---
多数のパラメータを持つ人工ニューロラルネットワークで構成されるコンピュータ言語モデル。
ラベルなしテキストを使用して自己教師あり学習または半教師あり学習で学習を行う

## 言語モデルの役割
---
言語モデルの基本的な役割は入力されたテキスト(トークン)に続く次の単語(トークン)を予測すること。あるテキストに対する出力の作成


## LLMの特徴
---
 * パラメータが非常に多い。
 * パラメータが多いと、計算コストやメモリ使用量が増大する
 * メモリ使用量や計算コストを抑えるために量子化というパラメータを低ビット幅で整数形式で表現し直す技術がある
 * 汎化性能が高い
 * パラメータ数と汎化性能の高さは必ずしも正の相関関係があるわけではないが、大規模言語モデルはパラメータが増えると正解率が高くなる特徴を持つ

## Transformer
---
ニューラルネットワークアーキテクチャの一種。
RNN系のモデルに比べて長期記憶を扱うことが得意
ChatGPTのベースを構築している
Attention = 注意機構という層を使用する
近年は画像処理にも使われ始めた(VIT)

### 自己注意機構
---
文中の単語が他の単語とどのように関連しているかを計算する仕組み。
これにより文中の長距離の依存関係を効率的に捉えられるようになった。
各要素と他の要素との関連性を計算し、重要な情報に注意を向けることで文脈理解を行う

### 並列計算
---
RNNのように逐次処理をする必要がなく、入力を一度に処理出来るため、大規模言語モデルを学習する際の計算効率が高い。

### 高い表現力
---
多層化しやすく、自己注意機構を重ねることで表現力の高いデータを学習できる

## 自己教師あり学習
---
 テキストの一部を隠してそれを予測しながら学習を行う。
  ラベル付きデータが少ない。またはラベル付けが大変な場合に用いる手法。テキストデータを用意するだけ


## 大規模言語モデルの評価
---
大規模言語モデルの性能は以下の３つの要素がわればおおよそ予想できる
	
1. 学習データサイズ
2. パラメータサイズ
3. 計算能力(1病患に浮動小数点計算を何回出来るか)


## ハルシネーション
---
間違った情報を出力してしまう減少。これを軽減するためにRAGという技術が開発された

### temperature
---
LLMで文章を生成する際にtemperatureというパラメータを調整すると、どの程度次の単語をランダムにサンプリングするか調整できる。

temperatureを高くすると確率分布が平坦になり、生成される文に多様性が生まれる。文章の一貫性が下がる
低くすると、尖った確率分布となり高確率の単語を優先して選択するため文章の一貫性を保ちやすくなる。創造性や多様性は抑えられる

### RAG
---
外部ドキュメントを検索し、そこから得た情報を取り込んで出力を生成する技術

## プロンプト
---
### OpenAIのベストプラクティス

1. より関連性の高い回答を得るには、クエリに詳細を含めてください
2. モデルにペルソナを採用するよう依頼する
3. 区切り文字を使用して、入力の異なる部分を明確に示します
4. タスクを完了するために必要な手順を指定します
5. 例を提供する
6. 出力の希望の長さを指定します


### システム開発におけるLLM
---
LLMをシステムに組み込みたい時、まずはLangChainを使用することを考える
https://github.com/langchain-ai/langchain