# Gradient
created: 2025-02-05 00:59
tags: #DeepLearning #Gradient

## Feeling Notes
---
- 
## References
---
- 

## 機械学習における学習とは？
---
損失関数を最小にする損失関数の変数(パラメータ)を求めること

## 勾配
---
関数Eをベクトル(w = [w1, w2, w3,...])で微分した結果。
言い換えると、関数Eを各変数(w1, w2, w3など)で偏微分した結果をベクトルにしたもの
変数が1個のみでwがスカラーの場合は微分係数と呼ぶ。
ある関数を微分して得られる関数を導関数と呼ぶ

勾配は、関数が最も急激に増加する方向を表した値

### 勾配の大切なポイント
---
- 勾配は、学習データX, yとパラメータwの関数
- 勾配を計算する時、X,yは定数扱いされる。変数はwである
- 勾配はベクトル(変数は2個以上という前提)
- 損失関数はスカラー

## 勾配が関数を最も急激に増加する方向を示す理由
---
勾配が関数を最も急激に増加させる方向を表す理由は、**勾配が関数の増加率が最大となる方向を指す**からです。これは、**多変数関数の微分の性質**と**線形近似**に基づいて説明できます。

---
### **1. 一変数関数での直感的な理解**

まず、一変数関数 f(x)f(x) を考えます。  
f(x)f(x) の微分 f′(x)f'(x) は、関数の変化率を表し、その符号が増減を決定します。

- f′(x)>0f'(x) > 0 のとき、f(x)f(x) は増加する
- f′(x)<0f'(x) < 0 のとき、f(x)f(x) は減少する

微分の大きさが大きいほど、増減の速さも大きくなります。

---

### **2. 多変数関数に拡張**

次に、多変数関数 f(x1,x2,...,xn)f(x_1, x_2, ..., x_n) を考えます。  
この場合、各変数について偏微分を求めたものを並べたベクトルが**勾配（gradient）**です：

$∇f(x)=(∂f∂x1,∂f∂x2,…,∂f∂xn)\nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right)$

ここで、点 xx の近くで関数 f(x)f(x) の増加を考えると、**テイラー展開の一次近似**を使って次のように表せます。

$f(x+Δx)≈f(x)+∇f(x)⋅Δxf(x + \Delta x) \approx f(x) + \nabla f(x) \cdot \Delta x$

ここで、$∇f(x)⋅Δx\nabla f(x) \cdot \Delta x$ は、勾配と移動ベクトル Δx\Delta x の内積を表します。

---

### **3. 内積の性質を考える**

内積の式：

$∇f(x)⋅Δx=∥∇f(x)∥∥Δx∥cos⁡θ\nabla f(x) \cdot \Delta x = \|\nabla f(x)\| \|\Delta x\| \cos \theta$

ここで：

- $∥∇f(x)∥\|\nabla f(x)\|$ は勾配の大きさ（ノルム）
- $∥Δx∥\|\Delta x\|$ は移動量の大きさ
- θ\theta は勾配と移動ベクトルのなす角

この式を見ると、関数の増加率を最大にするには、$cos⁡θ=1\cos \theta = 1$（つまり、勾配と同じ方向に進む）ときに最大値を取る**ことがわかります。  
つまり、**勾配の方向に進むと、関数は最も急激に増加する**のです。

---

### **4. まとめ**

- **勾配は偏微分を並べたベクトルであり、各方向の変化率を表す。**
- **関数の変化は内積（勾配と移動ベクトルの積）で表される。**
- **勾配と同じ方向に進むと、増加率が最大になる（cos⁡θ=1\cos \theta = 1 のとき）。**
- **逆に、勾配の逆方向に進めば、関数は最も急激に減少する（勾配降下法）。**

この性質を利用して、勾配降下法では「勾配の逆方向に移動する」ことで関数を最小化するのです！