---
tags:
  - "#DeepLearning"
  - "#RNN"
created:
  - 2025-02-08 22:08
---

# RNN
---

## Feeling Notes
---
-
## Literature Notes
---
- 
## References
---
-  https://docs.google.com/presentation/d/1mi4oH281BmWDbAXQWh7BBlVHaXKsXajIIR-YHdLAWlE/edit#slide=id.g3193c5ab41a_0_0
- [Recurrent Neural Networks (RNNs), Clearly Explained!!!](https://www.youtube.com/watch?v=AsNTP8Kwu80)
- [GRU (Gated Recurrent Unit, **ゲート付き再帰ユニット**](https://cvml-expertguide.net/terms/dl/rnn/gru/)

# RNNとは
---
入力や出力に可変長のデータを使用できるネットワーク
時系列データ(音、自然言語)を扱うときに使用される。

自然言語を扱う場合は、コンテキスト(文脈や背景)から「語」の違いを理解している。

機械学習は周期的(ほぼ一定の間隔をおいて、同じことが繰り返されるさま)なデータを学習することが苦手
**状態**を持つ。 I -> am -> happy.

> Hint: 時系列データは分解すると、周期的なデータが現れることがある。
> 一般的には、時系列データは以下の3つの成分で構成される
> T: 傾向的成分 y = x
> S: 季節変動成分(周期的な成分) y = xπ
> I: 不規則変動成分(ノイズの成分)
>
 T + S + I として分析するのを**加法モデル**
 T * S * I として分析するのを**乗法モデル**
 と呼ぶ


## RNNの構造
---
- 時系列データを扱うためには状態を持っておくと便利
- 中間層では前の時間の出力と今の時間を入力として今の時間の出力を行う。これを再帰する(回帰結合層)
- 前の時間の出力を入力として加えていることで時刻感の影響を考慮したNNになっている。
- TensorflowではSimpleRNNで通常のRNN中間層を扱える。
- **回帰結合層では、何度回帰しても重みやバイアスの値の個数は変わらない。。ただ値が更新されるだけ**

次元：[ サンプル数、タイムステップ、特徴量 ]

### タイムステップとは
時系列データを任意の1まとまりとして扱う時の大きさ。
何個区切りで時系列データを扱うかを決める。

例：タイムステップ5
[0, 1, 2, 3, 4], [1, 2, 3, 4, 5], [2, 3, 4, 5, 6]

## RNNの特徴
---
1. 前の時間の出力を次の時間の入力に渡すため、記憶のような機能を持たせることができる。
2. これまでのような単純なバックプロパゲーションが使えない。変わりにBPTT(back propagation through time)という重みの更新方法を使用する
3. 勾配爆発や勾配消失が起きやすい

## ゲート機構
---
**情報を選択的に記憶するため長期的な記憶のような機能と過学習を抑える機能を持たせる仕組み**
時系列データを勾配爆発や勾配消失をなるべく起こさないようにするネットワーク
LSTMやGRUといったものがある

### LSTM
---
忘却ゲート：過去の情報の内、どの情報を忘れさせるかをコントロール
入力ゲート：新しい情報をどの程度、記憶に刻むかをコントロール
出力ゲート：情報をどれだけ出力するかをコントロール

これらを使ってメモリーセルという記憶のような仕組みを構築している

### GRU
---
LSTMを簡略化したモデル。LSTMよりも構造がシンプルで計算負荷が小さい
更新ゲート：どの程度これまでの状態を保持し、新たな情報をどの程度取り入れるかかをコントロール
リセットゲート：過去の情報をどの程度忘れるかコントロール

**LSTMやGRUはRNNよりも長期記憶を持つことができる**

