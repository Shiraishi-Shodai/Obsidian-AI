
# Gradient descent method

created: 2025-02-05 01:09
tags: #DeepLearning #Gradient 

## Feeling Notes
---
- 
## References
---
- 

## 勾配降下法
---
勾配降下法とは、損失関数Eの最小値とそのEの最小値をとるwを求める方法。

| 長所             | 短所                                                  |
| -------------- | --------------------------------------------------- |
| 実装がシンプル        | 初期値によっては収束しなかったり、ローカルminに落ち込みグローバルminに到達しない         |
| 大規模なデータセットにも対応 | 必ず収束するとは限らない。損失関数が凸関数(すり鉢状)である場合は最小値に到達することが保証されている |
| 速い             | ローカルminに陥る可能性がある                                    |

### wの更新方法(アップデート方法)
---
1. 損失関数Eを求める
2. 適当にwを初期化する
3. 新しいwを求める
4. もし、wが0ベクトル(勾配が0)になったらループを抜ける。
5. wが0ベクトルでなければ新しいwをwとして手順3に戻る

### 勾配降下法で最小値にたどり着けないケース
---
1. 極小値に陥ってしまう
2. 鞍点がある関数や楕円形の関数では勾配を計算しても次に進む方向を見失ってしまう