# 🌌 螺旋型学習計画：Attention理解のためのチェックリスト

## 🎯 ゴール
- Attention層の仕組み（Q・K・V → Softmax → 重み付け）を理解する  
- Transformerエンコーダの流れをざっくり説明できる  
- 必要になった基礎（RNN, CNN, 数学）を寄り道しながら補強する  

---

## ✅ ステージ1: Attentionを体験する
- [x] GitHubリポジトリ ch08 のコードを実行してみる  
- [ ] Self-Attention の計算結果を確認（Q, K, V がどんな形をしているかを見る）  
- [ ] Attentionの可視化を試して「どの単語がどこに注目しているか」を体感する  
- [ ] 疑問を書き出す（例：「なぜSoftmax？」「なぜ内積？」など）  

---

## ✅ ステージ2: 疑問から寄り道する
### 例1: Softmaxが気になったら
- [ ] Softmax関数の実装をNumPyで確認  
- [ ] Softmax + クロスエントロピー誤差の逆伝播をノートで導出（短くでOK）  
- [ ] 小さな入力を使って数値的に確認（数式と一致するか）  

### 例2: 「系列処理って何？」と思ったら
- [ ] GitHubリポジトリ ch06 のRNNコードを実行  
- [ ] 隠れ状態が時間でどう更新されるかを確認  
- [ ] 勾配消失のイメージを図やメモで整理  

### 例3: 「CNNとの違いは？」と思ったら
- [ ] GitHubリポジトリ ch05 のCNNコードを実行  
- [ ] 畳み込みとAttentionの違いを比較（局所特徴 vs 全体依存関係）  

---

## ✅ ステージ3: Attentionを深掘り
- [ ] Multi-Head Attention のコードを実行  
- [ ] 「なぜ複数のヘッドが必要か」を自分なりに説明する  
- [ ] 位置エンコーディングを実装 or 可視化して理解  
- [ ] Transformerエンコーダの1層の流れを整理（入力 → Self-Attention → FFN → 出力）  

---

## ✅ ステージ4: 理解を定着させる
- [ ] 「Attentionはどんな問題を解決するのか」を説明できるようにする  
- [ ] 小さな例（2〜3単語）で手計算してみる（QK^T → Softmax → V）  
- [ ] ノートやObsidianに「理解したこと・まだ曖昧なこと」を残す  
- [ ] 曖昧な部分が出てきたら再度寄り道（螺旋を回す）  

---

## 🚀 最終ゴール
- [ ] Attentionの数式を紙に書ける  
- [ ] Transformerエンコーダの流れを図で説明できる  
- [ ] 「RNNとAttentionの違い」を言葉で説明できる  
- [ ] 「Attentionは何が嬉しいのか？」を自分なりに答えられる  
