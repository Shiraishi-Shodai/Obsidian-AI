---
tags:
  - Attention
  - Transformer
created:
  - 2025-08-23 23:29
---


## Feeling Notes
---
- 
## Literature Notes
---
- 
## References
---
- 

# わからないこと
---

- [x]  Attentionってようは何？
- [ ]  AttentionとTransformerの違いは？
- [ ]  AttentionとRNNの違いは？
- [x]  Attentionってなんで Q/K/V の3つに分ける必要があるの？
- [x]  QKVって何？
- [ ]  Attention(Q, K, V)の数式って何を表しているの？
	- [ ] Attention(Q, K, V)の数式でdkでスケーリングするのはなぜ？
	- [ ] QKVはどのようにベクトル化している？