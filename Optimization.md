

# Optimization

created: 2025-02-05 01:01
tags: #DeepLearning 

## Feeling Notes
---
- 
## References
---
- 

## 最適化
---
パラメータwの更新方法

### 勾配降下法以外の最適化関数
---
1. モーメンタム：勾配に慣性のようなものを加える。極小値に留まりにくくなる。凹凸が小さい損失関数に有効？betaを大きくすると慣性が大きくなる
2. AdaGrad : 最小値に近づくにつれ(勾配が0に近づくにつれ)学習係数を小さくし、進む距離を短くしていく